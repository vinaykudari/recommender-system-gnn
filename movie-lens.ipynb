{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "9972e09a-9660-40d6-a049-65912c2ce915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10\n",
    "from sklearn import metrics\n",
    "from tensorly import decomposition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.functional import tensordot\n",
    "from torch import nn, optim, Tensor\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.typing import Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e6a064db-b27e-449f-94a4-94a88236193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('data/movie-lens/ml-1m')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RATING_THRESHOLD = 3.\n",
    "N_USERS = 200\n",
    "N_ITEMS = 500\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "970a1ec8-e57f-4836-a61e-03ddb3ccf056",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\n",
    "    BASE_PATH/'users.dat',\n",
    "    sep='::',\n",
    "    header=None,\n",
    "    engine='python',\n",
    "    encoding='latin-1',\n",
    "    # usecols=[0, 1, 2],\n",
    "    # names=['a', 'b', 'c'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75ed8db4-b180-47aa-8482-99a73888e194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3      4\n",
       "0  1  F   1  10  48067\n",
       "1  2  M  56  16  70072\n",
       "2  3  M  25  15  55117\n",
       "3  4  M  45   7  02460\n",
       "4  5  M  25  20  55455"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "200e5325-f5f2-42a3-a446-08b3c75d4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(data, thresh):\n",
    "    ratings = data['adj_mat']\n",
    "    ratings[(ratings < thresh)] = 0\n",
    "    ratings[(ratings >= thresh)] = 1\n",
    "    data['adj_mat'] = ratings\n",
    "    return data\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, n_users=100, n_items=200):\n",
    "        self.root_dir = root_dir\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.transform = transform\n",
    "        self._load()\n",
    "        self._to_graph()\n",
    "        \n",
    "    def _read_table(self, path, cols, nrows=None, usecols=None):\n",
    "        df = pd.read_table(\n",
    "            path,\n",
    "            sep='::',\n",
    "            header=None,\n",
    "            engine='python',\n",
    "            encoding='latin-1',\n",
    "            usecols=usecols,\n",
    "            names=cols,\n",
    "            nrows=nrows,\n",
    "        )\n",
    "        return df\n",
    "        \n",
    "    def _load(self):\n",
    "        self.movies = self._read_table(\n",
    "            path=self.root_dir/'movies.dat',\n",
    "            cols=['movie_id', 'title', 'genres'],\n",
    "\n",
    "        )\n",
    "        self.users = self._read_table(\n",
    "            path=self.root_dir/'users.dat',\n",
    "            cols=['user_id', 'gender', 'age', 'occupation', 'zip'],\n",
    "            nrows=self.n_users,\n",
    "\n",
    "        )\n",
    "        self.ratings = self._read_table(\n",
    "            path=self.root_dir/'ratings.dat',\n",
    "            usecols=[0, 1, 2],\n",
    "            cols=['user_id', 'movie_id', 'rating'],\n",
    "\n",
    "        )\n",
    "        self.df = pd.merge(\n",
    "            pd.merge(self.ratings, self.users), \n",
    "            self.movies,\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= self.data.shape[0], 'Index out of range'\n",
    "        return self.data.iloc[idx, :]\n",
    "    \n",
    "    def _to_graph(self):\n",
    "        self.adj_mat = pd.pivot_table(\n",
    "            data=ds.ratings.merge(\n",
    "                ds.users, \n",
    "                left_on='user_id', \n",
    "                right_on='user_id',\n",
    "            )[self.ratings.columns], \n",
    "            index='user_id',\n",
    "            columns='movie_id',\n",
    "            values='rating',\n",
    "        )\n",
    "        self.adj_mat = self.adj_mat.fillna(0)\n",
    "        self.adj_mat = torch.tensor(self.adj_mat.values, device=DEVICE)\n",
    "        self.n_users, self.n_items = self.adj_mat.shape\n",
    "        \n",
    "        self.data = Data(\n",
    "            adj_mat=self.adj_mat,\n",
    "            raw_edge_index=self.adj_mat.clone(),\n",
    "            ratings=self.ratings,\n",
    "            users=self.users['user_id'],\n",
    "            items=self.movies['movie_id'],\n",
    "        )\n",
    "        \n",
    "        if self.transform:\n",
    "            self.data = self.transform(self.data)\n",
    "            \n",
    "    def _split(self, ratio=0.8):\n",
    "        n_edges = self.n_users * self.n_items\n",
    "        # why?\n",
    "        num_train_replaced = round((1-ratio) * n_edges)\n",
    "        num_val_show = round((1-ratio) * n_edges)\n",
    "\n",
    "        user_mask = np.random.randint(0, self.n_users, num_train_replaced)\n",
    "        movie_mask = np.random.randint(0, self.n_items, num_train_replaced)\n",
    "        \n",
    "        val_user_mask = np.random.choice(user_mask, num_val_show)\n",
    "        val_movie_mask = np.random.choice(movie_mask, num_val_show)\n",
    "\n",
    "        train_mask = torch.ones(self.n_users, self.n_items)\n",
    "        train_mask[user_mask, movie_mask] = 0\n",
    "\n",
    "        val_mask = train_mask.clone()\n",
    "        val_mask[val_user_mask, val_movie_mask] = 1\n",
    "\n",
    "        test_mask = torch.ones_like(train_mask)\n",
    "\n",
    "        return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "b7f6b59d-653c-4ddc-ada7-d63de6138434",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MovieLensDataset(\n",
    "    root_dir=BASE_PATH,\n",
    "    transform=transform_ratings,\n",
    "    n_users=N_USERS,\n",
    "    n_items=N_ITEMS,\n",
    ")\n",
    "train_mask, val_mask, test_mask = ds._split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "8f1b4578-2aa8-43ff-bab5-3fa7d26d8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self, n_users, n_items, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # sparse matrix to adjacency matrix : users X items\n",
    "        adj_mat = torch.zeros(self.n_users, self.n_items, device=x.device)\n",
    "        adj_mat[edge_index[:, 0], edge_index[:, 1]] = 1\n",
    "        \n",
    "        user_neighbour_count = adj_mat.sum(axis=1)\n",
    "        item_neighbour_count = adj_mat.sum(axis=0)\n",
    "        \n",
    "        weights = adj_mat / torch.sqrt(\n",
    "            user_neighbour_count.repeat(self.n_items, 1).T * item_neighbour_count.repeat(self.n_users, 1),\n",
    "        )\n",
    "        weights = torch.nan_to_num(weights, nan=0)\n",
    "        \n",
    "        user_embeddings = x[:self.n_users]\n",
    "        item_embeddings = x[self.n_users:]\n",
    "        out= torch.concat(\n",
    "            (weights.T @ user_embeddings, weights @ item_embeddings),\n",
    "            axis=0,\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "0eff8916-d141-4729-b84f-c40cd1709aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_layers, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=n_users + n_items,\n",
    "            embedding_dim=embed_dim,\n",
    "        )\n",
    "        \n",
    "        # experiment: try xavier initialization?\n",
    "        nn.init.normal_(self.embeddings.weight, std=0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            LightGCNConv(\n",
    "                n_users=n_users,\n",
    "                n_items=n_items,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                LightGCNConv(\n",
    "                    n_users=n_users,\n",
    "                    n_items=n_items,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.device = DEVICE\n",
    "        self.convs.to(DEVICE)\n",
    "        \n",
    "    def reset_params(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "        embed_lis = []\n",
    "        # print(f'edge_index: {edge_index.shape}')\n",
    "        # adjacency matrix to sparse \n",
    "        edge_index = torch.nonzero(edge_index)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            if self.device is not None:\n",
    "                x = x.to(self.device)\n",
    "            embed_lis.append(x)\n",
    "        embed_lis = torch.stack(embed_lis)\n",
    "        \n",
    "        self.alpha = 1 / (1 + self.n_layers) * torch.ones(embed_lis.shape)\n",
    "        if self.device is not None:\n",
    "            self.alpha = self.alpha.to(self.device)\n",
    "            embed_lis = embed_lis.to(self.device)\n",
    "            \n",
    "        # sum along K layers\n",
    "        x = (embed_lis * self.alpha).sum(dim=0)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "8070107a-ecf0-4213-8f26-f1346c855c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ratings(data):\n",
    "    return binarize(data, thresh=RATING_THRESHOLD)\n",
    "\n",
    "def get_user_rating(model, users, data):\n",
    "    embeddings = model(\n",
    "        model.embeddings.weight.clone(),\n",
    "        data['adj_mat'],\n",
    "    )\n",
    "    user_embeddings = embeddings[:len(data['users'])]\n",
    "    item_embeddings = embeddings[len(data['users']):]\n",
    "    user_embeddings = user_embeddings[users.long()]\n",
    "    rating = model.sigmoid(torch.matmul(user_embeddings, item_embeddings.t()))\n",
    "    return rating\n",
    "\n",
    "def get_embedding(model, users, pos, neg, data, mask):\n",
    "    n_user = len(data['users'])\n",
    "    embeddings = model(\n",
    "        model.embeddings.weight.clone(),\n",
    "        data['adj_mat'] * mask,\n",
    "    )\n",
    "    all_user_embeddings = embeddings[:len(data['users'])]\n",
    "    all_item_embeddings = embeddings[len(data['users']):]\n",
    "    \n",
    "    user_embeddings = all_user_embeddings[users]\n",
    "    pos_embeddings = all_item_embeddings[pos]\n",
    "    neg_embeddings = all_item_embeddings[neg]\n",
    "    \n",
    "    users_emb_ego = model.embeddings(users)\n",
    "    pos_emb_ego = model.embeddings(pos + n_user)\n",
    "    neg_emb_ego = model.embeddings(neg + n_user)\n",
    "    \n",
    "    return user_embeddings, pos_embeddings, neg_embeddings, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
    "\n",
    "def _sample_pos_neg(data, mask, num_samples_per_user):\n",
    "    samples = []\n",
    "    all_items = set(range(len(data['items'])))\n",
    "    for user_index, user in enumerate(data['users']):\n",
    "        pos_items = set(\n",
    "            torch.nonzero(data['adj_mat'][user_index])[:, 0].tolist(),\n",
    "        )\n",
    "        unknown_items = all_items.difference(\n",
    "                set(\n",
    "                    torch.nonzero(\n",
    "                        data['raw_edge_index'][user_index],\n",
    "                    )[:, 0].tolist(),\n",
    "                ),\n",
    "        )\n",
    "        neg_items = all_items.difference(\n",
    "            set(pos_items),\n",
    "        ).difference(set(unknown_items))\n",
    "        \n",
    "        unmasked_items = set(torch.nonzero(mask[user_index])[:, 0].tolist())\n",
    "        \n",
    "        if len(unknown_items.union(pos_items)) == 0 or len(unknown_items.union(neg_items)) == 0:\n",
    "            continue\n",
    "            \n",
    "        for _ in range(num_samples_per_user):\n",
    "            if len(pos_items.intersection(unmasked_items)) == 0:\n",
    "                pos_item_index = random.choice(\n",
    "                    list(unknown_items.intersection(unmasked_items)))\n",
    "            else:\n",
    "                pos_item_index = random.choice(\n",
    "                    list(pos_items.intersection(unmasked_items)))\n",
    "            if len(neg_items.intersection(unmasked_items)) == 0:\n",
    "                neg_item_index = random.choice(\n",
    "                    list(unknown_items.intersection(unmasked_items)))\n",
    "            else:\n",
    "                neg_item_index = random.choice(\n",
    "                    list(neg_items.intersection(unmasked_items)))\n",
    "            samples.append((user_index, pos_item_index, neg_item_index))\n",
    "\n",
    "    return torch.tensor(samples, dtype=torch.int32)\n",
    "\n",
    "def sample_pos_neg(data, train_mask, val_mask, test_mask, num_samples_per_user):\n",
    "    train_samples = _sample_pos_neg(data, train_mask, num_samples_per_user)\n",
    "    val_samples = _sample_pos_neg(data, val_mask, num_samples_per_user)\n",
    "    test_samples = _sample_pos_neg(data, test_mask, num_samples_per_user)\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "def bpr_loss(model, users, pos, neg, data, mask):\n",
    "    assert len(users) == len(pos) and len(users) == len(neg)\n",
    "    (users_emb, pos_emb, neg_emb, \n",
    "    userEmb0,  posEmb0, negEmb0) = get_embedding(model, users.long(), pos.long(),\n",
    "                                                neg.long(), data, mask)\n",
    "    reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                        posEmb0.norm(2).pow(2)  +\n",
    "                        negEmb0.norm(2).pow(2))/float(len(users))\n",
    "    pos_scores = torch.mul(users_emb, pos_emb)\n",
    "    pos_scores = torch.sum(pos_scores, dim=1)\n",
    "    neg_scores = torch.mul(users_emb, neg_emb)\n",
    "    neg_scores = torch.sum(neg_scores, dim=1)\n",
    "    \n",
    "    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "    \n",
    "    return loss, reg_loss\n",
    "\n",
    "def personalized_topk(pred, K, user_indices, edge_index):\n",
    "    per_user_preds = collections.defaultdict(list)\n",
    "    for index, user in enumerate(user_indices):\n",
    "        per_user_preds[user.item()].append(pred[index].item())\n",
    "    precisions = 0.0\n",
    "    recalls = 0.0\n",
    "    for user, preds in per_user_preds.items():\n",
    "        while len(preds) < K:\n",
    "            preds.append(random.choice(range(edge_index.shape[1])))\n",
    "        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n",
    "        correct_preds = edge_index[user, top_items].sum().item()\n",
    "        total_pos = edge_index[user].sum().item()\n",
    "        precisions += correct_preds / K\n",
    "        recalls += correct_preds / total_pos if total_pos != 0 else 0\n",
    "    num_users = len(user_indices.unique())\n",
    "    return precisions / num_users, recalls / num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "c51e9db0-5d92-449c-a3da-dfb90b1ee456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Users: 200\n",
      "#Items: 3883\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    'num_samples_per_user': 500,\n",
    "    'num_users': 200,\n",
    "\n",
    "    'epochs': 100,\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.1,\n",
    "\n",
    "    'embedding_size': 64,\n",
    "    'num_layers': 5,\n",
    "    'K': 10,\n",
    "    'mf_rank': 8,\n",
    "\n",
    "    'minibatch_per_print': 100,\n",
    "    'epochs_per_print': 1,\n",
    "\n",
    "    'val_frac': 0.2,\n",
    "    'test_frac': 0.1,\n",
    "\n",
    "    'model_name': 'model.pth'\n",
    "}\n",
    "\n",
    "print(f'#Users: {n_users}')\n",
    "print(f'#Items: {n_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "be976c47-bf3e-4cdd-b5ac-5c0f8be8b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "samples_train, samples_val, samples_test = sample_pos_neg(\n",
    "    ds.data, train_mask,\n",
    "    val_mask, test_mask,\n",
    "    num_samples_per_user,\n",
    ")\n",
    "\n",
    "n_users = len(ds.data['users'].unique())\n",
    "n_items = len(ds.data['items'].unique())\n",
    "\n",
    "model = LightGCN(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_layers=5,\n",
    "    embed_dim=64,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "samples_train=samples_train.to(DEVICE)\n",
    "samples_val=samples_val.to(DEVICE)\n",
    "samples_test=samples_test.to(DEVICE)\n",
    "train_mask=train_mask.to(DEVICE)\n",
    "val_mask=val_mask.to(DEVICE)\n",
    "test_mask=test_mask.to(DEVICE)\n",
    "data = ds.data.to(DEVICE)\n",
    "\n",
    "num_samples_per_user = config_dict['num_samples_per_user']\n",
    "epochs = config_dict['epochs']\n",
    "batch_size = 128\n",
    "lr = config_dict['lr']\n",
    "weight_decay = config_dict['weight_decay']\n",
    "K = config_dict['K']\n",
    "\n",
    "print(f'#Training samples: {len(samples_train)}',\n",
    "      f'#Validation samples: {len(samples_val)}',\n",
    "      f'#Test samples: {len(samples_test)}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print('Optimizer:', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "53877b35-b14e-4dad-93fc-c723cb9ecd2b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the 0 epoch\n",
      "Training on epoch 0 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.695444, and regularization loss is 0.002296.\n",
      " Top K precision = 0.08163265306122447, recall = 0.006672166161428612.\n",
      "Training on epoch 0 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.695708, and regularization loss is 0.00256.\n",
      " Top K precision = 0.08229166666666664, recall = 0.006792840524689204.\n",
      "Training on epoch 0 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693827, and regularization loss is 0.00068.\n",
      " Top K precision = 0.07499999999999996, recall = 0.00590237523336384.\n",
      "Training on epoch 0 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.694242, and regularization loss is 0.001095.\n",
      " Top K precision = 0.10109890109890107, recall = 0.007651455726572287.\n",
      "Training on epoch 0 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.694031, and regularization loss is 0.000884.\n",
      " Top K precision = 0.0912087912087912, recall = 0.007285702573100896.\n",
      "Training on epoch 0 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693708, and regularization loss is 0.000561.\n",
      " Top K precision = 0.09797979797979793, recall = 0.008874566641957645.\n",
      "Training on epoch 0 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693526, and regularization loss is 0.000379.\n",
      " Top K precision = 0.08962264150943393, recall = 0.008004055483968788.\n",
      "Training on epoch 0 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693542, and regularization loss is 0.000395.\n",
      " Top K precision = 0.09326923076923074, recall = 0.007502304626255316.\n",
      "\n",
      "Training on 0 epoch completed.\n",
      " Average bpr_loss on train set is 0.005428 for the current epoch.\n",
      " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 1 epoch\n",
      "Training on epoch 1 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693391, and regularization loss is 0.000243.\n",
      " Top K precision = 0.09891304347826083, recall = 0.006028471604376972.\n",
      "Training on epoch 1 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693486, and regularization loss is 0.000339.\n",
      " Top K precision = 0.0842696629213483, recall = 0.007824153976499295.\n",
      "Training on epoch 1 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693335, and regularization loss is 0.000188.\n",
      " Top K precision = 0.08181818181818176, recall = 0.006660394665657368.\n",
      "Training on epoch 1 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693361, and regularization loss is 0.000214.\n",
      " Top K precision = 0.07894736842105263, recall = 0.00837889569715552.\n",
      "Training on epoch 1 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693292, and regularization loss is 0.000145.\n",
      " Top K precision = 0.08064516129032256, recall = 0.0066369252297699905.\n",
      "Training on epoch 1 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693209, and regularization loss is 6.2e-05.\n",
      " Top K precision = 0.09139784946236554, recall = 0.007965220906813996.\n",
      "Training on epoch 1 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693322, and regularization loss is 0.000175.\n",
      " Top K precision = 0.09687499999999993, recall = 0.008599547464771278.\n",
      "Training on epoch 1 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693312, and regularization loss is 0.000165.\n",
      " Top K precision = 0.09302325581395349, recall = 0.007566848050938242.\n",
      "\n",
      "Training on 1 epoch completed.\n",
      " Average bpr_loss on train set is 0.005422 for the current epoch.\n",
      " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 2 epoch\n",
      "Training on epoch 2 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
      " Top K precision = 0.09509803921568626, recall = 0.008124062733668624.\n",
      "Training on epoch 2 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693263, and regularization loss is 0.000116.\n",
      " Top K precision = 0.07578947368421048, recall = 0.007293845637104392.\n",
      "Training on epoch 2 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693209, and regularization loss is 6.2e-05.\n",
      " Top K precision = 0.07684210526315789, recall = 0.006964946971554681.\n",
      "Training on epoch 2 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693223, and regularization loss is 7.6e-05.\n",
      " Top K precision = 0.08804347826086956, recall = 0.006671250048076674.\n",
      "Training on epoch 2 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
      " Top K precision = 0.09999999999999996, recall = 0.007007779925355352.\n",
      "Training on epoch 2 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693222, and regularization loss is 7.5e-05.\n",
      " Top K precision = 0.08817204301075267, recall = 0.007040531546420476.\n",
      "Training on epoch 2 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.08666666666666667, recall = 0.006553304258703622.\n",
      "Training on epoch 2 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693184, and regularization loss is 3.7e-05.\n",
      " Top K precision = 0.0945054945054945, recall = 0.008579917661820091.\n",
      "\n",
      "Training on 2 epoch completed.\n",
      " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
      " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 3 epoch\n",
      "Training on epoch 3 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
      " Top K precision = 0.09176470588235291, recall = 0.007427067360133038.\n",
      "Training on epoch 3 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693243, and regularization loss is 9.6e-05.\n",
      " Top K precision = 0.07741935483870967, recall = 0.008109043408093092.\n",
      "Training on epoch 3 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.08659793814432987, recall = 0.005348715583213887.\n",
      "Training on epoch 3 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
      " Top K precision = 0.0824175824175824, recall = 0.0072957244289312715.\n",
      "Training on epoch 3 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693304, and regularization loss is 0.000157.\n",
      " Top K precision = 0.09255319148936166, recall = 0.007736271542289547.\n",
      "Training on epoch 3 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
      " Top K precision = 0.1010204081632653, recall = 0.008057232470093819.\n",
      "Training on epoch 3 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.08699999999999995, recall = 0.006373754708522301.\n",
      "Training on epoch 3 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
      " Top K precision = 0.09892473118279567, recall = 0.008148763445558073.\n",
      "\n",
      "Training on 3 epoch completed.\n",
      " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
      " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 4 epoch\n",
      "Training on epoch 4 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.09183673469387753, recall = 0.008144907397918663.\n",
      "Training on epoch 4 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.69318, and regularization loss is 3.3e-05.\n",
      " Top K precision = 0.08586956521739131, recall = 0.007512092659869004.\n",
      "Training on epoch 4 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.69315, and regularization loss is 2e-06.\n",
      " Top K precision = 0.07551020408163264, recall = 0.0068429577664522465.\n",
      "Training on epoch 4 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08761904761904758, recall = 0.007258440278047508.\n",
      "Training on epoch 4 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693177, and regularization loss is 3e-05.\n",
      " Top K precision = 0.08952380952380949, recall = 0.007161756645462964.\n",
      "Training on epoch 4 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
      " Top K precision = 0.09306930693069301, recall = 0.008948934774603562.\n",
      "Training on epoch 4 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693149, and regularization loss is 2e-06.\n",
      " Top K precision = 0.10322580645161286, recall = 0.007531364147028913.\n",
      "Training on epoch 4 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.69315, and regularization loss is 2e-06.\n",
      " Top K precision = 0.08124999999999996, recall = 0.007409640977101547.\n",
      "\n",
      "Training on 4 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 5 epoch\n",
      "Training on epoch 5 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08469387755102038, recall = 0.008017728417614368.\n",
      "Training on epoch 5 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.69315, and regularization loss is 3e-06.\n",
      " Top K precision = 0.08297872340425531, recall = 0.006644475043636895.\n",
      "Training on epoch 5 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.07979797979797977, recall = 0.006225764488327369.\n",
      "Training on epoch 5 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
      " Top K precision = 0.08681318681318682, recall = 0.0056884557116150284.\n",
      "Training on epoch 5 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08314606741573033, recall = 0.005366531196577096.\n",
      "Training on epoch 5 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09299999999999996, recall = 0.006850151937409237.\n",
      "Training on epoch 5 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.09795918367346933, recall = 0.008619825739066946.\n",
      "Training on epoch 5 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
      " Top K precision = 0.08659793814432988, recall = 0.006801451486190788.\n",
      "\n",
      "Training on 5 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999997, recall = 0.007558772127474773.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 6 epoch\n",
      "Training on epoch 6 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693149, and regularization loss is 2e-06.\n",
      " Top K precision = 0.08556701030927832, recall = 0.007779487484032948.\n",
      "Training on epoch 6 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.09569892473118279, recall = 0.007789236290221982.\n",
      "Training on epoch 6 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08936170212765954, recall = 0.0069566404480152465.\n",
      "Training on epoch 6 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09111111111111109, recall = 0.007293168793817873.\n",
      "Training on epoch 6 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08556701030927834, recall = 0.007933415596490525.\n",
      "Training on epoch 6 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08478260869565213, recall = 0.00819850309039379.\n",
      "Training on epoch 6 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 5e-06.\n",
      " Top K precision = 0.08041237113402061, recall = 0.007586516953819718.\n",
      "Training on epoch 6 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 4e-06.\n",
      " Top K precision = 0.10204081632653057, recall = 0.007849860926622395.\n",
      "\n",
      "Training on 6 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 7 epoch\n",
      "Training on epoch 7 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.07659574468085105, recall = 0.006513209619960801.\n",
      "Training on epoch 7 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08279569892473113, recall = 0.006209924817810673.\n",
      "Training on epoch 7 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.06888888888888887, recall = 0.006625636379471179.\n",
      "Training on epoch 7 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08865979381443297, recall = 0.007442973424517448.\n",
      "Training on epoch 7 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 5e-06.\n",
      " Top K precision = 0.09354838709677418, recall = 0.008371943158587533.\n",
      "Training on epoch 7 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.0870967741935483, recall = 0.0077288927627274535.\n",
      "Training on epoch 7 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08617021276595742, recall = 0.006303268920678827.\n",
      "Training on epoch 7 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08586956521739128, recall = 0.008357924185551783.\n",
      "\n",
      "Training on 7 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999993, recall = 0.007558772127474779.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 8 epoch\n",
      "Training on epoch 8 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.69315, and regularization loss is 3e-06.\n",
      " Top K precision = 0.09801980198019795, recall = 0.008584133340665703.\n",
      "Training on epoch 8 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.08571428571428569, recall = 0.00705563140272288.\n",
      "Training on epoch 8 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.08901098901098899, recall = 0.009050574692536199.\n",
      "Training on epoch 8 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 5e-06.\n",
      " Top K precision = 0.0989473684210526, recall = 0.008058109526788754.\n",
      "Training on epoch 8 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08850574712643673, recall = 0.00830310048497705.\n",
      "Training on epoch 8 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.07977528089887637, recall = 0.008425632766948892.\n",
      "Training on epoch 8 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.09791666666666664, recall = 0.008248974442865166.\n",
      "Training on epoch 8 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
      " Top K precision = 0.08659793814432987, recall = 0.008344390997146145.\n",
      "\n",
      "Training on 8 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999995, recall = 0.007558772127474777.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 9 epoch\n",
      "Training on epoch 9 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
      " Top K precision = 0.0978947368421052, recall = 0.0074374308797387585.\n",
      "Training on epoch 9 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.09462365591397845, recall = 0.007750117916668921.\n",
      "Training on epoch 9 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.08478260869565214, recall = 0.007739379180346581.\n",
      "Training on epoch 9 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08555555555555552, recall = 0.008274466363946924.\n",
      "Training on epoch 9 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09999999999999992, recall = 0.007921000826000415.\n",
      "Training on epoch 9 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 7e-06.\n",
      " Top K precision = 0.08686868686868683, recall = 0.007876374329456086.\n",
      "Training on epoch 9 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 5e-06.\n",
      " Top K precision = 0.08172043010752686, recall = 0.0073009417106799515.\n",
      "Training on epoch 9 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.08247422680412368, recall = 0.004706386318797253.\n",
      "\n",
      "Training on 9 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999991, recall = 0.007558772127474779.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 10 epoch\n",
      "Training on epoch 10 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.07755102040816322, recall = 0.005698523834181533.\n",
      "Training on epoch 10 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.07623762376237622, recall = 0.006588969907844876.\n",
      "Training on epoch 10 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
      " Top K precision = 0.0923076923076923, recall = 0.008766175796317742.\n",
      "Training on epoch 10 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.0824175824175824, recall = 0.0065541632683289225.\n",
      "Training on epoch 10 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 7e-06.\n",
      " Top K precision = 0.10108695652173906, recall = 0.0092545988493318.\n",
      "Training on epoch 10 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09, recall = 0.007774341863397491.\n",
      "Training on epoch 10 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.07526881720430108, recall = 0.005440852049458425.\n",
      "Training on epoch 10 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
      " Top K precision = 0.08736842105263157, recall = 0.007645454467169671.\n",
      "\n",
      "Training on 10 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999998, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 11 epoch\n",
      "Training on epoch 11 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08369565217391303, recall = 0.007458577429248584.\n",
      "Training on epoch 11 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.09545454545454544, recall = 0.009327601331808943.\n",
      "Training on epoch 11 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.07634408602150533, recall = 0.0069905740059259735.\n",
      "Training on epoch 11 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08085106382978721, recall = 0.006695398572011115.\n",
      "Training on epoch 11 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 3e-06.\n",
      " Top K precision = 0.0792079207920792, recall = 0.006624687168430536.\n",
      "Training on epoch 11 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.08958333333333333, recall = 0.007999189592475257.\n",
      "Training on epoch 11 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
      " Top K precision = 0.09199999999999997, recall = 0.007331976398967068.\n",
      "Training on epoch 11 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08080808080808075, recall = 0.0067442683828569956.\n",
      "\n",
      "Training on 11 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 12 epoch\n",
      "Training on epoch 12 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08989898989898985, recall = 0.00819365078226773.\n",
      "Training on epoch 12 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
      " Top K precision = 0.09999999999999998, recall = 0.008453468822919129.\n",
      "Training on epoch 12 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.07333333333333332, recall = 0.007163513850568035.\n",
      "Training on epoch 12 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.08124999999999998, recall = 0.005892603199496739.\n",
      "Training on epoch 12 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.09462365591397846, recall = 0.007352881561712447.\n",
      "Training on epoch 12 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.08494623655913977, recall = 0.006561062310080725.\n",
      "Training on epoch 12 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.09438202247191008, recall = 0.007454054267724496.\n",
      "Training on epoch 12 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
      " Top K precision = 0.08699999999999998, recall = 0.006440326131946224.\n",
      "\n",
      "Training on 12 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999991, recall = 0.007558772127474776.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 13 epoch\n",
      "Training on epoch 13 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
      " Top K precision = 0.09117647058823525, recall = 0.008071698090247832.\n",
      "Training on epoch 13 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.06739130434782607, recall = 0.006420325883349682.\n",
      "Training on epoch 13 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08105263157894735, recall = 0.007614734936150471.\n",
      "Training on epoch 13 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.08901098901098901, recall = 0.0071815934201057705.\n",
      "Training on epoch 13 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
      " Top K precision = 0.11290322580645155, recall = 0.008461222053109634.\n",
      "Training on epoch 13 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.10098039215686268, recall = 0.007568848949949857.\n",
      "Training on epoch 13 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.08469387755102038, recall = 0.007911339151565304.\n",
      "Training on epoch 13 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.08333333333333331, recall = 0.00652307824292311.\n",
      "\n",
      "Training on 13 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 14 epoch\n",
      "Training on epoch 14 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693155, and regularization loss is 7e-06.\n",
      " Top K precision = 0.09062499999999996, recall = 0.008965540422514769.\n",
      "Training on epoch 14 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.09247311827956987, recall = 0.0068370602504635525.\n",
      "Training on epoch 14 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
      " Top K precision = 0.08988764044943819, recall = 0.00716505462076226.\n",
      "Training on epoch 14 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.09278350515463912, recall = 0.008808926852626334.\n",
      "Training on epoch 14 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
      " Top K precision = 0.09021739130434779, recall = 0.007754714928801276.\n",
      "Training on epoch 14 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09009900990099005, recall = 0.008085391938299496.\n",
      "Training on epoch 14 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09374999999999999, recall = 0.008575094938789867.\n",
      "Training on epoch 14 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
      " Top K precision = 0.09090909090909086, recall = 0.010094056631176195.\n",
      "\n",
      "Training on 14 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 15 epoch\n",
      "Training on epoch 15 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.09578947368421052, recall = 0.007725994444796581.\n",
      "Training on epoch 15 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
      " Top K precision = 0.07799999999999999, recall = 0.006851698679998203.\n",
      "Training on epoch 15 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.09619047619047613, recall = 0.0082206278948608.\n",
      "Training on epoch 15 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
      " Top K precision = 0.08399999999999996, recall = 0.007234519984962112.\n",
      "Training on epoch 15 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
      " Top K precision = 0.08695652173913042, recall = 0.008852410241850257.\n",
      "Training on epoch 15 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.06914893617021273, recall = 0.006057568462101249.\n",
      "Training on epoch 15 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.0894230769230769, recall = 0.005993471166661919.\n",
      "Training on epoch 15 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
      " Top K precision = 0.09456521739130432, recall = 0.00685011101558974.\n",
      "\n",
      "Training on 15 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999991, recall = 0.007558772127474776.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 16 epoch\n",
      "Training on epoch 16 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.0828282828282828, recall = 0.007204884515932219.\n",
      "Training on epoch 16 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.09117647058823532, recall = 0.006597297488934557.\n",
      "Training on epoch 16 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
      " Top K precision = 0.0770833333333333, recall = 0.007186718295128771.\n",
      "Training on epoch 16 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
      " Top K precision = 0.08210526315789471, recall = 0.007481962323343867.\n",
      "Training on epoch 16 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
      " Top K precision = 0.0913978494623656, recall = 0.0068219332699598545.\n",
      "Training on epoch 16 minibatch 501/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.09072164948453607, recall = 0.006107687707654535.\n",
      "Training on epoch 16 minibatch 601/782 completed\n",
      " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
      " Top K precision = 0.09797979797979793, recall = 0.007554184204578115.\n",
      "Training on epoch 16 minibatch 701/782 completed\n",
      " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
      " Top K precision = 0.10490196078431363, recall = 0.007373070624142808.\n",
      "\n",
      "Training on 16 epoch completed.\n",
      " Average bpr_loss on train set is 0.00542 for the current epoch.\n",
      " Training top K precision = 0.08899999999999995, recall = 0.007558772127474779.\n",
      " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
      " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
      "\n",
      "Training on the 17 epoch\n",
      "Training on epoch 17 minibatch 1/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
      " Top K precision = 0.08118811881188116, recall = 0.007146403274251794.\n",
      "Training on epoch 17 minibatch 101/782 completed\n",
      " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
      " Top K precision = 0.08585858585858583, recall = 0.008503967486887207.\n",
      "Training on epoch 17 minibatch 201/782 completed\n",
      " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
      " Top K precision = 0.09690721649484531, recall = 0.009319649044037466.\n",
      "Training on epoch 17 minibatch 301/782 completed\n",
      " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
      " Top K precision = 0.11789473684210522, recall = 0.01004186228775068.\n",
      "Training on epoch 17 minibatch 401/782 completed\n",
      " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
      " Top K precision = 0.09340659340659337, recall = 0.0070285625356435975.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/3932884462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# print(f'users: {users.shape}, pos: {pos.shape}, neg: {neg.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         loss, reg_loss = bpr_loss(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/3104728151.py\u001b[0m in \u001b[0;36mbpr_loss\u001b[0;34m(model, users, pos, neg, data, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     (users_emb, pos_emb, neg_emb, \n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0muserEmb0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mposEmb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegEmb0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                                                 neg.long(), data, mask)\n\u001b[1;32m     85\u001b[0m     reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/3104728151.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(model, users, pos, neg, data, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mn_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'users'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     embeddings = model(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adj_mat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/181165389.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/195833487.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0muser_neighbour_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mitem_neighbour_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         )\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0muser_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_tracked = []\n",
    "train_topks = []\n",
    "val_topks = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Training on the {} epoch'.format(epoch))\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    # Shuffle the order of rows.\n",
    "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
    "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current_batch = samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
    "        # Shuffle the order of rows.\n",
    "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
    "        users = current_batch[:, 0]\n",
    "        pos = current_batch[:, 1]\n",
    "        neg = current_batch[:, 2]\n",
    "        \n",
    "        # print(f'users: {users.shape}, pos: {pos.shape}, neg: {neg.shape}') \n",
    "        loss, reg_loss = bpr_loss(\n",
    "            model, users, \n",
    "            pos, neg, \n",
    "            data, train_mask,\n",
    "        )\n",
    "        reg_loss = reg_loss * weight_decay\n",
    "        loss = loss + reg_loss\n",
    "        loss_sum += loss.detach()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % config_dict['minibatch_per_print'] == 0:\n",
    "            all_users = torch.linspace(start=0,\n",
    "                                       end=n_users - 1, steps=n_users).long()\n",
    "            user_indices = current_batch[:, 0]\n",
    "            user_indices = user_indices.repeat(2).long()\n",
    "            item_indices = torch.cat(\n",
    "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
    "            \n",
    "            pred = get_user_rating(\n",
    "                model, all_users, data,\n",
    "            )[user_indices, item_indices]\n",
    "            truth = data['adj_mat'][user_indices, item_indices]\n",
    "            topk_precision, topk_recall = personalized_topk(\n",
    "                pred, K, user_indices, data['adj_mat'],\n",
    "            )\n",
    "\n",
    "            print('Training on epoch {} minibatch {}/{} completed\\n'.format(epoch, batch_idx+1,\n",
    "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
    "                  'bpr_loss on current minibatch is {}, and regularization loss is {}.\\n'.format(round(float(loss.detach().cpu()), 6),\n",
    "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
    "                  'Top K precision = {}, recall = {}.'.format(topk_precision, topk_recall))\n",
    "\n",
    "    if epoch % config_dict['epochs_per_print'] == 0:\n",
    "        epochs_tracked.append(epoch)\n",
    "\n",
    "        # evaluation on both the trainisng and validation set\n",
    "        model.eval()\n",
    "        # predict on the training set\n",
    "        users = samples_train[:, 0:1]\n",
    "        user_indices = samples_train[:, 0]\n",
    "        user_indices = user_indices.repeat(2).long()\n",
    "        item_indices = torch.cat(\n",
    "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
    "        pred = get_user_rating(\n",
    "            model,\n",
    "            users[:,0],\n",
    "            data,\n",
    "        )[user_indices, item_indices]\n",
    "        truth = data['adj_mat'][users.long()[:,0]][user_indices, item_indices]\n",
    "        train_topk_precision, train_topk_recall = personalized_topk(pred, K, user_indices, data['adj_mat'])\n",
    "        train_topks.append((train_topk_precision, train_topk_recall))\n",
    "\n",
    "        # predict on the validation set\n",
    "        users_val = samples_val[:, 0:1]\n",
    "        pos_val = samples_val[:, 1:2]\n",
    "        neg_val = samples_val[:, 2:3]\n",
    "\n",
    "        loss_val, reg_loss_val = bpr_loss(\n",
    "            model, users_val, pos_val, neg_val, data, val_mask,\n",
    "        )\n",
    "        reg_loss_val = reg_loss_val * weight_decay\n",
    "\n",
    "        # predict on the validation set\n",
    "        user_indices = samples_val[:, 0]\n",
    "        user_indices = user_indices.repeat(2).long()\n",
    "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
    "        pred_val = get_user_rating(\n",
    "            model,\n",
    "            users_val[:,0],\n",
    "            data,\n",
    "        )[user_indices, item_indices]\n",
    "        truth_val = data['adj_mat'][users_val.long()[:,0]][user_indices, item_indices]\n",
    "        val_topk_precision, val_topk_recall = personalized_topk(\n",
    "            pred_val, K, user_indices, data['adj_mat'],\n",
    "        )\n",
    "        val_topks.append((val_topk_precision, val_topk_recall))\n",
    "\n",
    "        print('\\nTraining on {} epoch completed.\\n'.format(epoch),\n",
    "              'Average bpr_loss on train set is {} for the current epoch.\\n'.format(round(float(loss_sum/len(samples_train)), 6)),\n",
    "              'Training top K precision = {}, recall = {}.\\n'.format(train_topk_precision, train_topk_recall),\n",
    "              'Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n'.format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
    "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
    "              'Validation top K precision = {}, recall = {}.\\n'.format(val_topk_precision, val_topk_recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
