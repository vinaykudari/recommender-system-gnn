{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9972e09a-9660-40d6-a049-65912c2ce915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10\n",
    "from sklearn import metrics\n",
    "from tensorly import decomposition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.functional import tensordot\n",
    "from torch import nn, optim, Tensor\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.typing import Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e6a064db-b27e-449f-94a4-94a88236193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('data/movie-lens/ml-1m')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RATING_THRESHOLD = 3.\n",
    "N_USERS = 200\n",
    "N_ITEMS = 500\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "970a1ec8-e57f-4836-a61e-03ddb3ccf056",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\n",
    "    BASE_PATH/'users.dat',\n",
    "    sep='::',\n",
    "    header=None,\n",
    "    engine='python',\n",
    "    encoding='latin-1',\n",
    "    # usecols=[0, 1, 2],\n",
    "    # names=['a', 'b', 'c'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75ed8db4-b180-47aa-8482-99a73888e194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3      4\n",
       "0  1  F   1  10  48067\n",
       "1  2  M  56  16  70072\n",
       "2  3  M  25  15  55117\n",
       "3  4  M  45   7  02460\n",
       "4  5  M  25  20  55455"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "200e5325-f5f2-42a3-a446-08b3c75d4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(data, thresh):\n",
    "    ratings = data['adj_mat']\n",
    "    ratings[(ratings < thresh)] = 0\n",
    "    ratings[(ratings >= thresh)] = 1\n",
    "    data['adj_mat'] = ratings\n",
    "    return data\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, n_users=100, n_items=200):\n",
    "        self.root_dir = root_dir\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.transform = transform\n",
    "        self._load()\n",
    "        self._to_graph()\n",
    "        \n",
    "    def _read_table(self, path, cols, nrows=None, usecols=None):\n",
    "        df = pd.read_table(\n",
    "            path,\n",
    "            sep='::',\n",
    "            header=None,\n",
    "            engine='python',\n",
    "            encoding='latin-1',\n",
    "            usecols=usecols,\n",
    "            names=cols,\n",
    "            nrows=nrows,\n",
    "        )\n",
    "        return df\n",
    "        \n",
    "    def _load(self):\n",
    "        self.movies = self._read_table(\n",
    "            path=self.root_dir/'movies.dat',\n",
    "            cols=['movie_id', 'title', 'genres'],\n",
    "\n",
    "        )\n",
    "        self.users = self._read_table(\n",
    "            path=self.root_dir/'users.dat',\n",
    "            cols=['user_id', 'gender', 'age', 'occupation', 'zip'],\n",
    "            nrows=self.n_users,\n",
    "\n",
    "        )\n",
    "        self.ratings = self._read_table(\n",
    "            path=self.root_dir/'ratings.dat',\n",
    "            usecols=[0, 1, 2],\n",
    "            cols=['user_id', 'movie_id', 'rating'],\n",
    "\n",
    "        )\n",
    "        self.df = pd.merge(\n",
    "            pd.merge(self.ratings, self.users), \n",
    "            self.movies,\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= self.data.shape[0], 'Index out of range'\n",
    "        return self.data.iloc[idx, :]\n",
    "    \n",
    "    def _to_graph(self):\n",
    "        adj_mat = pd.pivot_table(\n",
    "            data=self.ratings, \n",
    "            index='user_id',\n",
    "            columns='movie_id',\n",
    "            values='rating',\n",
    "        )\n",
    "        adj_mat = adj_mat.fillna(0)\n",
    "        adj_mat = torch.tensor(adj_mat.values, device=DEVICE)\n",
    "        \n",
    "        self.n_users, self.n_items = adj_mat.shape\n",
    "        \n",
    "        self.data = Data(\n",
    "            adj_mat=adj_mat,\n",
    "            raw_edge_index=adj_mat.clone(),\n",
    "            ratings=self.ratings,\n",
    "            users=self.users['user_id'],\n",
    "            items=self.movies['movie_id'],\n",
    "        )\n",
    "        \n",
    "        if self.transform:\n",
    "            self.data = self.transform(self.data)\n",
    "            \n",
    "    def _split(self, ratio=0.8):\n",
    "        n_edges = self.n_users * self.n_items\n",
    "        # why?\n",
    "        num_train_replaced = round((1-ratio) * n_edges)\n",
    "        num_val_show = round((1-ratio) * n_edges)\n",
    "\n",
    "        user_mask = np.random.randint(0, self.n_users, num_train_replaced)\n",
    "        movie_mask = np.random.randint(0, self.n_items, num_train_replaced)\n",
    "        \n",
    "        val_user_mask = np.random.choice(user_mask, num_val_show)\n",
    "        val_movie_mask = np.random.choice(movie_mask, num_val_show)\n",
    "\n",
    "        train_mask = torch.ones(self.n_users, self.n_items)\n",
    "        train_mask[user_mask, movie_mask] = 0\n",
    "\n",
    "        val_mask = train_mask.clone()\n",
    "        val_mask[val_user_mask, val_movie_mask] = 1\n",
    "\n",
    "        test_mask = torch.ones_like(train_mask)\n",
    "\n",
    "        return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8f1b4578-2aa8-43ff-bab5-3fa7d26d8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self, n_users, n_items, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # sparse matrix to adjacency matrix : users X items\n",
    "        adj_mat = torch.zeros(self.n_users, self.n_items, device=x.device)\n",
    "        adj_mat[edge_index[:, 0], edge_index[:, 1]] = 1\n",
    "        \n",
    "        user_neighbour_count = adj_mat.sum(axis=1)\n",
    "        item_neighbout_count = adj_mat.sum(axis=0)\n",
    "        \n",
    "        weights = adj_mat / torch.sqrt(\n",
    "            user_neighbour_count.repeat(self.n_items, 1).T * item_neightbor_counts.repeat(self.n_users, 1),\n",
    "        )\n",
    "        weights = torch.nan_to_num(weights, nan=0)\n",
    "        \n",
    "        user_embeddings = x[:self.n_users]\n",
    "        item_embeddings = x[self.n_users:]\n",
    "        out= torch.concat(\n",
    "            (weights.T @ user_embeddings, weights @ item_embeddings),\n",
    "            axis=0,\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "0eff8916-d141-4729-b84f-c40cd1709aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_layers, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=n_users + n_items,\n",
    "            embedding_dim=embed_dim,\n",
    "        )\n",
    "        \n",
    "        # experiment: try xavier initialization?\n",
    "        nn.init.normal_(self.embeddings.weight, std=0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            LightGCNConv(\n",
    "                n_users=n_users,\n",
    "                n_items=n_items,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                LightGCNConv(\n",
    "                    n_users=n_users,\n",
    "                    n_items=n_items,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.device = DEVICE\n",
    "        self.convs.to(DEVICE)\n",
    "        \n",
    "    def reset_params(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "        embed_lis = []\n",
    "        print(f'edge_index: {edge_index.shape}')\n",
    "        # adjacency matrix to sparse \n",
    "        edge_index = torch.nonzero(edge_index)\n",
    "        for i in range(self.n_layers):\n",
    "            print(f'embed before: {x.shape}')\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            print(f'embed after: {x.shape}')\n",
    "            if self.device is not None:\n",
    "                x = x.to(self.device)\n",
    "            embed_lis.append(x)\n",
    "        embed_lis = torch.stack(embed_lis)\n",
    "        \n",
    "        self.alpha = 1 / (1 + self.n_layers) * torch.ones(embed_lis.shape)\n",
    "        if self.device is not None:\n",
    "            self.alpha = self.alpha.to(self.device)\n",
    "            embed_lis = embed_lis.to(self.device)\n",
    "            \n",
    "        # sum along K layers\n",
    "        x = (embed_lis * self.alpha).sum(dim=0)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8070107a-ecf0-4213-8f26-f1346c855c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ratings(data):\n",
    "    return binarize(data, thresh=RATING_THRESHOLD)\n",
    "\n",
    "def get_user_rating(model, users, data):\n",
    "    embeddings = model(\n",
    "        model.embeddings.weight.clone(),\n",
    "        data['adj_mat'],\n",
    "    )\n",
    "    user_embeddings = embeddings[:len(data['users'])]\n",
    "    item_embeddings = embeddings[len(data['users']):]\n",
    "    user_embeddings = user_embeddings[users.long()]\n",
    "    rating = model.f(torch.matmul(user_embeddings, item_embeddings.t()))\n",
    "    return rating\n",
    "\n",
    "def get_embedding(model, users, pos, neg, data, mask):\n",
    "    n_user = len(data['users'])\n",
    "    embeddings = model(\n",
    "        model.embeddings.weight.clone(),\n",
    "        data['adj_mat'] * mask,\n",
    "    )\n",
    "    user_embeddings = embeddings[:len(data['users'])]\n",
    "    item_embeddings = embeddings[len(data['users']):]\n",
    "    \n",
    "    user_embeddings = user_embeddings[users]\n",
    "    pos_embeddings = user_embeddings[pos]\n",
    "    neg_embeddings = user_embeddings[neg]\n",
    "    \n",
    "    users_emb_ego = model.emb(users)\n",
    "    pos_emb_ego = model.embeddings(pos + n_user)\n",
    "    neg_emb_ego = model.embeddings(neg + n_user)\n",
    "    \n",
    "    return user_embeddings, pos_embeddings, neg_embeddings, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
    "\n",
    "def _sample_pos_neg(data, mask, num_samples_per_user):\n",
    "    samples = []\n",
    "    all_items = set(range(len(data['items'])))\n",
    "    for user_index, user in enumerate(data['users']):\n",
    "        pos_items = set(\n",
    "            torch.nonzero(data['adj_mat'][user_index])[:, 0].tolist(),\n",
    "        )\n",
    "        unknown_items = all_items.difference(\n",
    "                set(\n",
    "                    torch.nonzero(\n",
    "                        data['raw_edge_index'][user_index],\n",
    "                    )[:, 0].tolist(),\n",
    "                ),\n",
    "        )\n",
    "        neg_items = all_items.difference(\n",
    "            set(pos_items),\n",
    "        ).difference(set(unknown_items))\n",
    "        \n",
    "        unmasked_items = set(torch.nonzero(mask[user_index])[:, 0].tolist())\n",
    "        \n",
    "        if len(unknown_items.union(pos_items)) == 0 or len(unknown_items.union(neg_items)) == 0:\n",
    "            continue\n",
    "            \n",
    "        for _ in range(num_samples_per_user):\n",
    "            if len(pos_items.intersection(unmasked_items)) == 0:\n",
    "                pos_item_index = random.choice(\n",
    "                    list(unknown_items.intersection(unmasked_items)))\n",
    "            else:\n",
    "                pos_item_index = random.choice(\n",
    "                    list(pos_items.intersection(unmasked_items)))\n",
    "            if len(neg_items.intersection(unmasked_items)) == 0:\n",
    "                neg_item_index = random.choice(\n",
    "                    list(unknown_items.intersection(unmasked_items)))\n",
    "            else:\n",
    "                neg_item_index = random.choice(\n",
    "                    list(neg_items.intersection(unmasked_items)))\n",
    "            samples.append((user_index, pos_item_index, neg_item_index))\n",
    "\n",
    "    return torch.tensor(samples, dtype=torch.int32)\n",
    "\n",
    "def sample_pos_neg(data, train_mask, val_mask, test_mask, num_samples_per_user):\n",
    "    train_samples = _sample_pos_neg(data, train_mask, num_samples_per_user)\n",
    "    val_samples = _sample_pos_neg(data, val_mask, num_samples_per_user)\n",
    "    test_samples = _sample_pos_neg(data, test_mask, num_samples_per_user)\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "def bpr_loss(model, users, pos, neg, data, mask):\n",
    "    assert len(users) == len(pos) and len(users) == len(neg)\n",
    "    (users_emb, pos_emb, neg_emb, \n",
    "    userEmb0,  posEmb0, negEmb0) = get_embedding(model, users.long(), pos.long(),\n",
    "                                                neg.long(), data, mask)\n",
    "    reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                        posEmb0.norm(2).pow(2)  +\n",
    "                        negEmb0.norm(2).pow(2))/float(len(users))\n",
    "    pos_scores = torch.mul(users_emb, pos_emb)\n",
    "    pos_scores = torch.sum(pos_scores, dim=1)\n",
    "    neg_scores = torch.mul(users_emb, neg_emb)\n",
    "    neg_scores = torch.sum(neg_scores, dim=1)\n",
    "    \n",
    "    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "    \n",
    "    return loss, reg_loss\n",
    "\n",
    "def personalized_topk(pred, K, user_indices, edge_index):\n",
    "    per_user_preds = collections.defaultdict(list)\n",
    "    for index, user in enumerate(user_indices):\n",
    "        per_user_preds[user.item()].append(pred[index].item())\n",
    "    precisions = 0.0\n",
    "    recalls = 0.0\n",
    "    for user, preds in per_user_preds.items():\n",
    "        while len(preds) < K:\n",
    "            preds.append(random.choice(range(edge_index.shape[1])))\n",
    "        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n",
    "        correct_preds = edge_index[user, top_items].sum().item()\n",
    "        total_pos = edge_index[user].sum().item()\n",
    "        precisions += correct_preds / K\n",
    "        recalls += correct_preds / total_pos if total_pos != 0 else 0\n",
    "    num_users = len(user_indices.unique())\n",
    "    return precisions / num_users, recalls / num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c672259f-94c3-4273-8cc2-4eb7558c2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MovieLensDataset(\n",
    "    root_dir=BASE_PATH,\n",
    "    transform=transform_ratings,\n",
    "    n_users=N_USERS,\n",
    "    n_items=N_ITEMS,\n",
    ")\n",
    "train_mask, val_mask, test_mask = ds._split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c51e9db0-5d92-449c-a3da-dfb90b1ee456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Users: 200\n",
      "#Items: 3883\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    'num_samples_per_user': 500,\n",
    "    'num_users': 200,\n",
    "\n",
    "    'epochs': 100,\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.1,\n",
    "\n",
    "    'embedding_size': 64,\n",
    "    'num_layers': 5,\n",
    "    'K': 10,\n",
    "    'mf_rank': 8,\n",
    "\n",
    "    'minibatch_per_print': 100,\n",
    "    'epochs_per_print': 1,\n",
    "\n",
    "    'val_frac': 0.2,\n",
    "    'test_frac': 0.1,\n",
    "\n",
    "    'model_name': 'model.pth'\n",
    "}\n",
    "n_users = len(ds.data['users'].unique())\n",
    "n_items = len(ds.data['items'].unique())\n",
    "print(f'#Users: {n_users}')\n",
    "print(f'#Items: {n_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0b46e394-7d93-4405-9042-1f85ac7e5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, samples_val, samples_test = sample_pos_neg(\n",
    "    ds.data, train_mask,\n",
    "    val_mask, test_mask,\n",
    "    500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d0b4c8ca-e661-4df9-9983-49ea3102ecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_layers=5,\n",
    "    embed_dim=64,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "samples_train=samples_train.to(DEVICE)\n",
    "samples_val=samples_val.to(DEVICE)\n",
    "samples_test=samples_test.to(DEVICE)\n",
    "train_mask=train_mask.to(DEVICE)\n",
    "val_mask=val_mask.to(DEVICE)\n",
    "test_mask=test_mask.to(DEVICE)\n",
    "data = ds.data.to(DEVICE)\n",
    "\n",
    "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
    "epochs = config_dict[\"epochs\"]\n",
    "batch_size = config_dict[\"batch_size\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "\n",
    "K = config_dict[\"K\"]\n",
    "\n",
    "print(f'#Training samples: {len(samples_train)}',\n",
    "      f'#Validation samples: {len(samples_val)}',\n",
    "      f'#Test samples: {len(samples_test)}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print('Optimizer:', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "53877b35-b14e-4dad-93fc-c723cb9ecd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the 0 epoch\n",
      "edge_index: torch.Size([6040, 3706])\n",
      "embed before: torch.Size([4083, 64])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3079 is out of bounds for dimension 0 with size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/2843004828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         loss, reg_loss = bpr_loss(\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/967922292.py\u001b[0m in \u001b[0;36mbpr_loss\u001b[0;34m(model, users, pos, neg, data, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     (users_emb, pos_emb, neg_emb, \n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0muserEmb0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mposEmb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegEmb0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                                                 neg.long(), data, mask)\n\u001b[1;32m     85\u001b[0m     reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/967922292.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(model, users, pos, neg, data, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mn_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'users'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     embeddings = model(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adj_mat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/3576002301.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'embed before: {x.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'embed after: {x.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_69748/1486912080.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# sparse matrix to adjacency matrix : users X items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0madj_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0madj_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0muser_neighbour_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3079 is out of bounds for dimension 0 with size 200"
     ]
    }
   ],
   "source": [
    "epochs_tracked = []\n",
    "train_topks = []\n",
    "val_topks = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training on the {} epoch\".format(epoch))\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    # Shuffle the order of rows.\n",
    "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
    "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current_batch = samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
    "        # Shuffle the order of rows.\n",
    "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
    "        users = current_batch[:, 0]\n",
    "        pos = current_batch[:, 1]\n",
    "        neg = current_batch[:, 2]\n",
    "\n",
    "        loss, reg_loss = bpr_loss(\n",
    "            model, users, \n",
    "            pos, neg, \n",
    "            data, train_mask,\n",
    "        )\n",
    "        reg_loss = reg_loss * weight_decay\n",
    "        loss = loss + reg_loss\n",
    "        loss_sum += loss.detach()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
    "            all_users = torch.linspace(start=0,\n",
    "                                       end=n_users - 1, steps=n_users).long()\n",
    "            user_indices = current_batch[:, 0]\n",
    "            user_indices = user_indices.repeat(2).long()\n",
    "            item_indices = torch.cat(\n",
    "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
    "            pred = get_user_rating(\n",
    "                model,\n",
    "                all_users,\n",
    "                data,\n",
    "            )[user_indices, item_indices]\n",
    "            truth = data['edge_index'][user_indices, item_indices]\n",
    "            topk_precision, topk_recall = personalized_topk(\n",
    "                pred, K, user_indices, data[\"edge_index\"],\n",
    "            )\n",
    "\n",
    "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
    "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
    "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
    "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
    "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
    "\n",
    "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
    "        epochs_tracked.append(epoch)\n",
    "\n",
    "        # evaluation on both the trainisng and validation set\n",
    "        model.eval()\n",
    "        # predict on the training set\n",
    "        users = samples_train[:, 0:1]\n",
    "        user_indices = samples_train[:, 0]\n",
    "        user_indices = user_indices.repeat(2).long()\n",
    "        item_indices = torch.cat(\n",
    "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
    "        pred = get_user_rating(\n",
    "            model,\n",
    "            users[:,0],\n",
    "            data,\n",
    "        )[user_indices, item_indices]\n",
    "        truth = data[\"edge_index\"][users.long()[:,0]][user_indices, item_indices]\n",
    "        train_topk_precision, train_topk_recall = personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
    "        train_topks.append((train_topk_precision, train_topk_recall))\n",
    "\n",
    "        # predict on the validation set\n",
    "        users_val = samples_val[:, 0:1]\n",
    "        pos_val = samples_val[:, 1:2]\n",
    "        neg_val = samples_val[:, 2:3]\n",
    "\n",
    "        loss_val, reg_loss_val = bpr_loss(\n",
    "            model, users_val, pos_val, neg_val, data, val_mask,\n",
    "        )\n",
    "        reg_loss_val = reg_loss_val * weight_decay\n",
    "\n",
    "        # predict on the validation set\n",
    "        user_indices = samples_val[:, 0]\n",
    "        user_indices = user_indices.repeat(2).long()\n",
    "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
    "        pred_val = get_user_rating(\n",
    "            model,\n",
    "            users_val[:,0],\n",
    "            data,\n",
    "        )[user_indices, item_indices]\n",
    "        truth_val = data[\"edge_index\"][users_val.long()[:,0]][user_indices, item_indices]\n",
    "        val_topk_precision, val_topk_recall = personalized_topk(\n",
    "            pred_val, K, user_indices, data[\"edge_index\"],\n",
    "        )\n",
    "        val_topks.append((val_topk_precision, val_topk_recall))\n",
    "\n",
    "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
    "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
    "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
    "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
    "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
    "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
